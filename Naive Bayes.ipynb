{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teach Computer how to classify Messages\n",
    "\n",
    "We will use multinominal Naive Bayes Alogorithm \n",
    "The dataset will have 5572 SMS Messages\n",
    "\n",
    "Tha dataset is put together by UCI Machine Learning REpository.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
    "\n",
    "Can be download from \n",
    "https://dq-content.s3.amazonaws.com/433/SMSSpamCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "ham     86.593683\n",
      "spam    13.406317\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data is separated by tab ,used sep='\\t'\n",
    "# Data doesn't have header used Header = None\n",
    "# Created column lables.\n",
    "# Spam means spam\n",
    "# Ham Means non-spam\n",
    "\n",
    "spamdataset = pd.read_csv('SMSSpamCollection', sep='\\t',header=None)\n",
    "names = ['Label','SMS']\n",
    "spamdataset.columns = names\n",
    "\n",
    "# Number of Rows = 5572 and No.of columns = 2\n",
    "print(spamdataset.shape)\n",
    "\n",
    "# % of Spam Messages\n",
    "# 87% messages are non-spam \n",
    "# 13% messages are spam\n",
    "\n",
    "print(spamdataset['Label'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training and Test Set\n",
    "\n",
    "It's very helpful to first think of a way of testing how well it works. When creating software (a spam filter is software), a good rule of thumb is that designing the test comes before creating the software. If we write the software first, then it's tempting to come up with a biased test just to make sure the software passes it.\n",
    "\n",
    "Once our spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter, we're first going to split our dataset into two categories:\n",
    "\n",
    " - A __training set__, which we'll use to \"train\" the computer how to classify messages.\n",
    " - A __test set__, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "\n",
    "For this project, our goal is to create a spam filter that classifies new messages with an accuracy greater than 80% — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label                                                SMS\n",
      "1078   ham                       Yep, by the pretty sculpture\n",
      "4028   ham      Yes, princess. Are you going to make me moan?\n",
      "958    ham                         Welp apparently he retired\n",
      "4642   ham                                            Havent.\n",
      "4674   ham  I forgot 2 ask ü all smth.. There's a card on ...\n",
      "4458\n",
      "ham     86.54105\n",
      "spam    13.45895\n",
      "Name: Label, dtype: float64\n",
      "ham     86.804309\n",
      "spam    13.195691\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Use the frac=1 parameter to randomize the entire dataset\n",
    "random_sample = spamdataset.sample(random_state = 1,frac=1)\n",
    "print(random_sample.head(5))\n",
    "\n",
    "#Calculate Index for split\n",
    "training_split_index = round(len(random_sample) * 0.8)\n",
    "print(training_split_index)\n",
    "\n",
    "## split the dataset into 2 , 80% in training and 20% Testing\n",
    "training_sample = random_sample.iloc[:training_split_index].reset_index(drop=True)\n",
    "\n",
    "## 20% in Testing\n",
    "testing_sample = random_sample.iloc[training_split_index:].reset_index(drop=True)\n",
    "\n",
    "# percentage of training and testing\n",
    "print(training_sample['Label'].value_counts(normalize=True)*100)\n",
    "print(testing_sample['Label'].value_counts(normalize=True)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentages of Training and Testing samples are similar to what we have in full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "To calculate all the probabilities required by the algorithm, we'll first need to perform a bit of data cleaning to bring the data in a formattimg hat will allow us to extract easily all the information we need.\n",
    "\n",
    "Split the text into columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                         Yep, by the pretty sculpture\n",
      "1        Yes, princess. Are you going to make me moan?\n",
      "2                           Welp apparently he retired\n",
      "3                                              Havent.\n",
      "4    I forgot 2 ask ü all smth.. There's a card on ...\n",
      "Name: SMS, dtype: object\n",
      "0                         yep  by the pretty sculpture\n",
      "1        yes  princess  are you going to make me moan \n",
      "2                           welp apparently he retired\n",
      "3                                              havent \n",
      "4    i forgot 2 ask ü all smth   there s a card on ...\n",
      "Name: SMS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Cleaning\n",
    "\n",
    "print(training_sample['SMS'].head(5))\n",
    "training_sample['SMS'] = training_sample['SMS'].str.replace('\\W',' ')\n",
    "training_sample['SMS'] = training_sample['SMS'].str.lower()\n",
    "print(training_sample['SMS'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Vocabulary\n",
    "\n",
    "Count the each words in the SMS column into a List and remove duplicate by SET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                    [yep, by, the, pretty, sculpture]\n",
      "1    [yes, princess, are, you, going, to, make, me,...\n",
      "2                      [welp, apparently, he, retired]\n",
      "3                                             [havent]\n",
      "4    [i, forgot, 2, ask, ü, all, smth, there, s, a,...\n",
      "Name: SMS, dtype: object\n",
      "72427\n",
      "After Removing dups 7783\n"
     ]
    }
   ],
   "source": [
    "# Split SMS column into words in \n",
    "training_sample['SMS']  = training_sample['SMS'].str.split()\n",
    "print(training_sample['SMS'].head(5))\n",
    "\n",
    "vocabulary = []\n",
    "### iterate through SMS colum\n",
    "\n",
    "for items in training_sample['SMS']:\n",
    "    for word in items:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "print(len(vocabulary))        \n",
    "\n",
    "#Transform List into SET to remove dups\n",
    "vocabularyset = set(vocabulary)\n",
    "vocabulary = list(vocabularyset)\n",
    "\n",
    "print(\"After Removing dups\" , len(vocabulary))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Training Set\n",
    "To create the dictionary we need for our training set, we can use the code below, where:\n",
    "\n",
    "We start by initializing a dictionary named __word_counts_per_sms__, where each key is a unique word (a string) from the vocabulary, and each value is a list of the length of training set, where each element in the list is a 0.\n",
    "\n",
    "The code __[0] * 5 outputs [0, 0, 0, 0, 0]__. So the code __[0] * len(training_set['SMS'])__ outputs a list of the length of __training_set['SMS']__, where each element in the list will be a 0.\n",
    "- We loop over training_set['SMS'] using at the same time the enumerate() function to get both the index and the SMS message (index and sms).\n",
    "\n",
    "    - Using a nested loop, we loop over sms (where sms is a list of strings, where each string represents a word in a message).\n",
    "        - We incremenent word_counts_per_sms[word][index] by 1.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(training_sample['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(training_sample['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: ticket, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Transform word count per sms into Dataframe\n",
    "\n",
    "print(\"done\")\n",
    "word_count = pd.DataFrame(word_counts_per_sms)\n",
    "print(word_count['ticket'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  0  00  000  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]  0   0    0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...  0   0    0   \n",
       "2   ham                    [welp, apparently, he, retired]  0   0    0   \n",
       "3   ham                                           [havent]  0   0    0   \n",
       "4   ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...  0   0    0   \n",
       "\n",
       "   000pes  008704050406  0089  01223585334  02 ...  zindgi  zoe  zogtorius  \\\n",
       "0       0             0     0            0   0 ...       0    0          0   \n",
       "1       0             0     0            0   0 ...       0    0          0   \n",
       "2       0             0     0            0   0 ...       0    0          0   \n",
       "3       0             0     0            0   0 ...       0    0          0   \n",
       "4       0             0     0            0   0 ...       0    0          0   \n",
       "\n",
       "   zouk  zyada  é  ú1  ü  〨ud  鈥  \n",
       "0     0      0  0   0  0    0  0  \n",
       "1     0      0  0   0  0    0  0  \n",
       "2     0      0  0   0  0    0  0  \n",
       "3     0      0  0   0  0    0  0  \n",
       "4     0      0  0   0  2    0  0  \n",
       "\n",
       "[5 rows x 7785 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_clean = pd.concat([training_sample, word_count], axis=1)\n",
    "training_set_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4458\n"
     ]
    }
   ],
   "source": [
    "print(len(training_sample['SMS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Constants First\n",
    "\n",
    "Some of the terms in the four equations above will have the same value for every new message. As a start, let's first calculate:\n",
    "\n",
    "- P(Spam) and P(Ham)\n",
    "- __N__Spam, __N__Ham, __N__Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam DataSet  (600, 7785)\n",
      "Non Spam DataSet  (3858, 7785)\n",
      "Length Spam DataSet  600\n",
      "Length Non Spam DataSet  3858\n",
      "Length of Training Dataset 4458\n"
     ]
    }
   ],
   "source": [
    "# Calculate p(spam) - Probablity of spam\n",
    "\n",
    "spam_dataset  = training_set_clean[training_set_clean['Label'] == 'spam']\n",
    "ham_dataset = training_set_clean[training_set_clean['Label'] == 'ham']\n",
    "\n",
    "print(\"Spam DataSet \", spam_dataset.shape)\n",
    "print(\"Non Spam DataSet \", ham_dataset.shape)\n",
    "\n",
    "print(\"Length Spam DataSet \", len(spam_dataset))\n",
    "print(\"Length Non Spam DataSet \", len(ham_dataset))\n",
    "print(\"Length of Training Dataset\", len(training_set_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probablity of Spam  :  0.13458950201884254\n",
      "Probablity of non Spam  :  0.8654104979811574\n"
     ]
    }
   ],
   "source": [
    "### Spam and Non SPam numbers\n",
    "p_spam = len(spam_dataset) / len(training_set_clean)\n",
    "p_ham = len(ham_dataset) / len(training_set_clean)\n",
    "\n",
    "print (\"Probablity of Spam  : \", p_spam)\n",
    "print (\"Probablity of non Spam  : \", p_ham)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary  :  7783\n"
     ]
    }
   ],
   "source": [
    "alpha = 1 \n",
    "n_vocabulary = len(vocabulary)\n",
    "print(\"Length of Vocabulary  : \", n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_spam :  15190\n",
      "n_ham :  57237\n"
     ]
    }
   ],
   "source": [
    "## n_spam\n",
    "no_of_words = spam_dataset['SMS'].apply(len)\n",
    "n_spam = no_of_words.sum()\n",
    "print(\"n_spam : \", n_spam )\n",
    "\n",
    "## n_ham\n",
    "no_of_words_ham = ham_dataset['SMS'].apply(len)\n",
    "n_ham = no_of_words_ham.sum()\n",
    "print(\"n_ham : \", n_ham )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Parameters\n",
    "\n",
    "Now that we have the constant terms calculated above, we can move on with calculating the parameters \n",
    " and \n",
    "-  Each parameter will thus be a conditional probability value associated with each word in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize 2 dictionaries \n",
    "\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_dataset[word].sum()  \n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "    \n",
    "    n_word_given_ham = ham_dataset[word].sum()   # ham_dataset already defined in a cell above\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying A New Message\n",
    "\n",
    "Now that we've calculated all the constants and parameters we need, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "- Takes in as input a new message (w1, w2, ..., wn)\n",
    "- Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "    - If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "    - If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "    - If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    '''    \n",
    "    This is where we calculate:\n",
    "\n",
    "    p_spam_given_message = ?\n",
    "    p_ham_given_message = ?\n",
    "    '''    \n",
    "    \n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    print(p_spam)\n",
    "    print(p_ham)\n",
    "    print(len(parameters_spam))\n",
    "    print(len(parameters_ham))\n",
    "    \n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13458950201884254\n",
      "0.8654104979811574\n",
      "7783\n",
      "7783\n",
      "P(Spam|message): 1.3481290211300841e-25\n",
      "P(Ham|message): 1.9368049028589875e-27\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13458950201884254\n",
      "0.8654104979811574\n",
      "7783\n",
      "7783\n",
      "P(Spam|message): 2.4372375665888117e-25\n",
      "P(Ham|message): 3.687530435009238e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "classify('Sounds good, Tom, then see u there')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the Spam Filter's Accuracy\n",
    "\n",
    " we'll change the __classify()__ function that we wrote previously to return the labels instead of printing them. Below, note that we now have return statements instead of print() functions:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_test_set(message):    \n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "    \n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "            \n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "    \n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1114, 3)\n",
      "  Label                                                SMS predict_msg\n",
      "0   ham          Later i guess. I needa do mcat study too.         ham\n",
      "1   ham             But i haf enuff space got like 4 mb...         ham\n",
      "2  spam  Had your mobile 10 mths? Update to latest Oran...        spam\n",
      "3   ham  All sounds good. Fingers . Makes it difficult ...         ham\n",
      "4   ham  All done, all handed in. Don't know if mega sh...         ham\n"
     ]
    }
   ],
   "source": [
    "## Now that we have a function that returns labels instead of \n",
    "## printing them, we can use it to create a new column in our test set.\n",
    "\n",
    "print(testing_sample.shape)\n",
    "\n",
    "testing_sample['predict_msg'] = testing_sample['SMS'].apply(classify_test_set)\n",
    "print(testing_sample.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we'll write a function to measure the accuracy of our spam filter to find out how well our spam filter does.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1100\n",
      "Incorrect: 14\n",
      "Accuracy: 0.9874326750448833\n"
     ]
    }
   ],
   "source": [
    "total = testing_sample.shape[0]\n",
    "correct = 0\n",
    "\n",
    "for row in testing_sample.iterrows():\n",
    "    row = row[1]\n",
    "    if (row['Label'] == row['predict_msg']):\n",
    "        correct += 1\n",
    "\n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy:', correct/total)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is close to 98.74%, which is really good. Our spam filter looked at 1,114 messages that it hasn't seen in training, and classified 1,100 correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
